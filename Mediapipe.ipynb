{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb87776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da96a6a7",
   "metadata": {},
   "source": [
    "# Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52554b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e3d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results= model.process(image)                         # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR CONVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "503c96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture (0)\n",
    "# Set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened ():\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        # Show to screen\n",
    "        cv2.imshow( 'OpenCV Feed', frame)\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "349f7204",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x: 0.42143386602401733\n",
       "y: 0.47879743576049805\n",
       "z: -0.9892539978027344\n",
       "visibility: 0.9998626112937927\n",
       ", x: 0.4507085382938385\n",
       "y: 0.42770856618881226\n",
       "z: -0.9444332122802734\n",
       "visibility: 0.9997002482414246\n",
       ", x: 0.46955519914627075\n",
       "y: 0.42897576093673706\n",
       "z: -0.9447585344314575\n",
       "visibility: 0.9996923804283142\n",
       ", x: 0.4852076470851898\n",
       "y: 0.4305424690246582\n",
       "z: -0.9446088075637817\n",
       "visibility: 0.999615490436554\n",
       ", x: 0.4000634551048279\n",
       "y: 0.42874830961227417\n",
       "z: -0.9153910875320435\n",
       "visibility: 0.9996906518936157\n",
       ", x: 0.38445308804512024\n",
       "y: 0.42951568961143494\n",
       "z: -0.9149736166000366\n",
       "visibility: 0.9997077584266663\n",
       ", x: 0.3716147840023041\n",
       "y: 0.4311687648296356\n",
       "z: -0.9156813621520996\n",
       "visibility: 0.9996845126152039\n",
       ", x: 0.5138581991195679\n",
       "y: 0.4578075706958771\n",
       "z: -0.6075328588485718\n",
       "visibility: 0.9996401071548462\n",
       ", x: 0.36448436975479126\n",
       "y: 0.4606562554836273\n",
       "z: -0.5225558280944824\n",
       "visibility: 0.9997338652610779\n",
       ", x: 0.45610904693603516\n",
       "y: 0.5378579497337341\n",
       "z: -0.8637858629226685\n",
       "visibility: 0.9998747110366821\n",
       ", x: 0.3924732208251953\n",
       "y: 0.538395345211029\n",
       "z: -0.8317187428474426\n",
       "visibility: 0.9998825192451477\n",
       ", x: 0.6420238018035889\n",
       "y: 0.7539339065551758\n",
       "z: -0.38266998529434204\n",
       "visibility: 0.9971082806587219\n",
       ", x: 0.27165424823760986\n",
       "y: 0.7533480525016785\n",
       "z: -0.3130979537963867\n",
       "visibility: 0.9978836178779602\n",
       ", x: 0.6947386264801025\n",
       "y: 1.1463983058929443\n",
       "z: -0.4009612202644348\n",
       "visibility: 0.14934855699539185\n",
       ", x: 0.23123760521411896\n",
       "y: 1.0645719766616821\n",
       "z: -0.2911105751991272\n",
       "visibility: 0.23564502596855164\n",
       ", x: 0.6853203773498535\n",
       "y: 1.4032466411590576\n",
       "z: -0.7923007011413574\n",
       "visibility: 0.03350287675857544\n",
       ", x: 0.21468372642993927\n",
       "y: 1.4065086841583252\n",
       "z: -0.8345619440078735\n",
       "visibility: 0.054899126291275024\n",
       ", x: 0.7038823366165161\n",
       "y: 1.4979252815246582\n",
       "z: -0.9210766553878784\n",
       "visibility: 0.04743025451898575\n",
       ", x: 0.19166673719882965\n",
       "y: 1.5054452419281006\n",
       "z: -0.97967129945755\n",
       "visibility: 0.0720924511551857\n",
       ", x: 0.6701346039772034\n",
       "y: 1.486104130744934\n",
       "z: -0.9965454936027527\n",
       "visibility: 0.08136386424303055\n",
       ", x: 0.21930214762687683\n",
       "y: 1.4950700998306274\n",
       "z: -1.0783212184906006\n",
       "visibility: 0.1211707666516304\n",
       ", x: 0.6537628769874573\n",
       "y: 1.4460632801055908\n",
       "z: -0.842138946056366\n",
       "visibility: 0.07825103402137756\n",
       ", x: 0.2386329472064972\n",
       "y: 1.456300139427185\n",
       "z: -0.896931529045105\n",
       "visibility: 0.1179765909910202\n",
       ", x: 0.5898946523666382\n",
       "y: 1.4274120330810547\n",
       "z: -0.0768246203660965\n",
       "visibility: 0.0008585554314777255\n",
       ", x: 0.34242039918899536\n",
       "y: 1.4269659519195557\n",
       "z: 0.08122760057449341\n",
       "visibility: 0.0008398168138228357\n",
       ", x: 0.5778073668479919\n",
       "y: 1.9626044034957886\n",
       "z: -0.054259635508060455\n",
       "visibility: 0.0015179504407569766\n",
       ", x: 0.3458186984062195\n",
       "y: 1.9620442390441895\n",
       "z: 0.031249765306711197\n",
       "visibility: 0.00043309462489560246\n",
       ", x: 0.5703186392784119\n",
       "y: 2.4408977031707764\n",
       "z: 0.519077718257904\n",
       "visibility: 0.00011495572107378393\n",
       ", x: 0.3480955958366394\n",
       "y: 2.444678783416748\n",
       "z: 0.43937745690345764\n",
       "visibility: 2.4513718017260544e-05\n",
       ", x: 0.5779137015342712\n",
       "y: 2.5200352668762207\n",
       "z: 0.5422836542129517\n",
       "visibility: 5.92587239225395e-05\n",
       ", x: 0.3487565815448761\n",
       "y: 2.526276111602783\n",
       "z: 0.460956871509552\n",
       "visibility: 5.9750385844381526e-05\n",
       ", x: 0.5231324434280396\n",
       "y: 2.598222255706787\n",
       "z: -0.018981903791427612\n",
       "visibility: 0.00011869847367051989\n",
       ", x: 0.37123537063598633\n",
       "y: 2.593893527984619\n",
       "z: -0.13486644625663757\n",
       "visibility: 0.00013709247286897153\n",
       "]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_landmarks.landmark # All the face_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0313abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pose = []\\nfor res in results.pose_landmarks.landmark:\\n    test = np.array([res.x, res.y, res.z, res.visibility])\\n    pose.append(test)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "    pose.append(test)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4af753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x,  res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten( ) if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8cf66fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1662,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keypoints(results).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e6d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeec987",
   "metadata": {},
   "source": [
    "# Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8db71f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for eexported data, numpy arrays\n",
    "DATA_PATH = os.path.join(os.getcwd(),\"MP_data\")\n",
    "\n",
    "# Actions trhat we try to detect\n",
    "actions = np.array(['hello', 'thanks', 'iloveyou'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5794cb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Programs\\\\Sign Recognition\\\\MP_data'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d57bf9b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# hello\n",
    "## 0\n",
    "## 1\n",
    "## 2\n",
    "\n",
    "#thanks\n",
    "## 1\n",
    "## 2\n",
    "## 3\n",
    "\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879fceb",
   "metadata": {},
   "source": [
    "# Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dff403ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Show to screen\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenCV Feed\u001b[39m\u001b[38;5;124m'\u001b[39m, image)\n\u001b[1;32m---> 31\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m     33\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollecting frames for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Video Number \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(action, sequence), (\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m12\u001b[39m), \n\u001b[0;32m     34\u001b[0m                cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.5\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m1\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "#                 print(results)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c88c5164",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eff8ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7eedecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a270a6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0, 'thanks': 1, 'iloveyou': 2}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d491c7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\Programs\\\\Sign Recognition\\\\MP_data\\\\hello\\\\6\\\\0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m window \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sequence_length):\n\u001b[1;32m----> 6\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_num\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     window\u001b[38;5;241m.\u001b[39mappend(res)\n\u001b[0;32m      8\u001b[0m sequences\u001b[38;5;241m.\u001b[39mappend(window)\n",
      "File \u001b[1;32mE:\\Programs\\Pythonev\\Tensorflowenv\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\Programs\\\\Sign Recognition\\\\MP_data\\\\hello\\\\6\\\\0.npy'"
     ]
    }
   ],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "320f87a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 30, 1662)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
